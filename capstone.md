Great! I’ll generate a Jekyll-compatible blog post in Markdown format with a clean layout and placeholders for screenshots. It will be styled with standard Jekyll post structure and designed to render beautifully on your GitHub Pages site at `/capstone`. I’ll get that ready for you shortly!

---
layout: post
title: "Commit Pro – Development Journey and Lessons Learned"
date: 2025-04-08
---

## Introduction

As part of my senior capstone project, I worked on **Commit Pro**, a team-based project that aimed to improve the quality of software commit documentation using modern tools. Commit Pro is a Chrome browser extension integrated with GitHub that automatically generates clear commit summaries and detects code refactorings in each commit. Our goal was to streamline the development workflow by providing developers and stakeholders with instant insights into code changes. In this blog post, I reflect on the project overview, my contributions as a backend developer, the agile process our team followed, the challenges we faced, lessons learned, and the outcome of this journey.

## Project Overview

Commit Pro was conceived to address a common pain point in software development: **poor commit messages and lack of contextual information**. Developers often write minimal or unclear commit messages, making it hard for others (and their future selves) to understand what changed and why. Our solution integrates directly into GitHub through a browser extension to automatically produce meaningful commit message summaries using AI. The extension also leverages a refactoring detection tool to highlight structural code changes (like refactors) and provides a web-based dashboard for additional repository metrics.

**Key features of Commit Pro include:**

- **AI-Generated Commit Summaries:** When a developer makes a commit, the extension uses a Large Language Model (LLM) to generate a concise summary of the changes. This summary emphasizes the intent and impact of the code changes, helping team members quickly grasp what the commit does.
- **Refactoring Detection:** We integrated the _RefactoringMiner_ API to analyze commit diffs for any refactoring operations (such as renaming a method or extracting a class). If refactorings are detected, the commit summary highlights them, ensuring that structural changes are documented.
- **Repository Metrics Dashboard:** In addition to the in-line commit information, Commit Pro offers a dashboard (built as a React web app) that displays various code quality metrics for the project’s repository. This includes metrics like Lines of Code (LOC), Cyclomatic Complexity (CC), Coupling, Cohesion, and other software quality indicators. The dashboard visualizes these metrics (e.g., via charts) and allows comparison between different commits or time periods.
- **Seamless GitHub Integration:** The Chrome extension injects the commit summaries and refactoring info directly into GitHub’s interface (e.g., on commit pages or pull request views). The dashboard is accessible via the extension or a link from the GitHub UI, and it uses the same backend to retrieve data.
- **Multi-Stakeholder Benefits:** While developers get immediate, improved commit documentation, other stakeholders benefit as well. Project managers can use the generated summaries and metrics for better progress reports and time tracking. QA engineers gain insights for impact analysis of changes (knowing which parts of the code were refactored or are more complex can help in testing). Even leadership and compliance officers can glean high-level trends in productivity and code quality over time.

Behind the scenes, the system’s architecture is composed of several interacting components. On the front end, we have the **Chrome Extension** (for GitHub UI integration) and the **React Dashboard**. On the back end, we developed a **Spring Boot API** server that acts as an API gateway to orchestrate between various services and external APIs. This backend handles requests from the extension and dashboard, then routes them to the appropriate modules or external services:

- The backend communicates with the **GitHub API** to fetch commit details and diffs.
- It calls an AI service (originally using OpenAI’s GPT models) to generate the commit message summaries from the diff and commit metadata.
- It uses the **RefactoringMiner** tool to analyze a commit’s diff (or a lightweight clone of the repo at that commit) to identify any refactoring operations.
- It interfaces with the **SciTools Understand** tool (via its API/CLI) to compute repository-level metrics for the dashboard.
- An **Authentication service** ensures secure access to external APIs (for example, managing OAuth tokens for GitHub and safeguarding the AI API key provided by the user).

This modular architecture allowed us to develop and test each piece (extension, dashboard, backend services) somewhat independently and then integrate them. We containerized the backend components for easy deployment, and the extension/dashboard share a common configuration to interact with the backend’s REST API. Overall, the Commit Pro project delivers an end-to-end solution: from code being committed, to automated analysis happening on the backend, and finally to insightful information being presented to users directly within their development workflow.

## My Role

On the Commit Pro team of five, I took on the role of a **Backend Developer**. I was primarily responsible for designing and implementing the server-side of our application and ensuring it effectively connected the front-end components with the various external services that power Commit Pro’s features. My contributions spanned several areas:

- **Backend Architecture & Development:** I helped design the overall system architecture, deciding how to split functionalities between the client (extension/dashboard) and server. I implemented the backend using **Java (Spring Boot)** to create RESTful APIs that the Chrome extension and React dashboard could call for data. This involved setting up controllers and service layers to handle requests such as “generate commit summary for commit X” or “retrieve latest metrics for repository Y”. I focused on making these APIs efficient and secure, given that they interact with third-party services and handle sensitive tokens.
- **API Integration (GitHub, LLM, RefactoringMiner, Understand):** A large part of my work was integrating external APIs and tools into our backend:
  - I implemented the connection to the **GitHub API** – enabling our system to pull commit data (like commit messages, diffs, author info) when needed. This included handling OAuth authentication flows so that users can authorize our tool to read their repository data. I ensured that once a user provides a GitHub token (or API key), our backend securely stores it and uses it for subsequent API calls.
  - For the **AI commit summary generation**, I integrated OpenAI’s API. I wrote the code to format commit diffs and metadata into a prompt that could be sent to the AI model (initially GPT-3.5-turbo). I also handled the response parsing to extract the summary. During development, I explored using **Spring AI** (a library to integrate LLMs into Spring applications) to simplify this process. While Spring AI offered a structured way to call AI models and even experiment with multiple models, it introduced complexity and some compatibility issues with our project. After some trials, we decided to implement the calls to the AI API directly and focus on prompt engineering to improve summary quality. I participated in refining the prompts – for example, adjusting wording and instructions given to the model to yield more concise and informative summaries. We also tried different models; at one point we switched to a smaller, faster model for testing, and later balanced between cost and quality for our final choice.
  - For **refactoring detection**, I integrated the RefactoringMiner tool into our pipeline. RefactoringMiner can analyze two versions of a codebase (or a commit diff) and identify refactoring operations. My responsibility was to supply it with the necessary data. One challenge we encountered was that RefactoringMiner’s API, when used with private repositories, needed GitHub authentication. We initially attempted to provide our stored GitHub token directly to RefactoringMiner, but it didn’t accept it due to how the tool expected the token (and possibly environment variable issues). I helped devise a workaround: performing a **shallow clone** of the repository at the specific commit we want to analyze. This means our backend pulls just that commit’s snapshot of the code (which is faster than a full clone) and then runs RefactoringMiner on the local code. This approach bypassed the need for RefactoringMiner to independently call GitHub, and it resolved the authentication problem. I wrote the logic to manage these operations and to parse the results (which refactorings were found) so we could include them in our output.
  - For **code metrics analysis**, I worked on integrating the **Understand** static analysis tool. Understand can produce detailed metrics about a codebase (like LOC, complexity, various object-oriented metrics). However, we faced an unexpected limitation: the educational license we used did not allow direct export of metrics via their API in the usual way. I took on the task of overcoming this. I ended up using a Python script in conjunction with Understand’s command-line interface (CLI). Essentially, the backend triggers a Python subprocess that runs Understand’s CLI commands to analyze the repository and output metrics to a CSV or stdout. I then capture that output and return it through our API to be displayed on the dashboard. It took several iterations to get this working, because initially the CLI output was coming up empty due to license restrictions. By combining approaches (having Understand generate an internal project file and then reading metrics from it via the Python API in segments), I managed to extract the needed data. This part of the project underscored the importance of flexibility with multi-language solutions – our backend is primarily Java, but using Python for this specific task was the most effective route.
- **Containerization and DevOps:** I containerized the backend services using Docker. Each major component (the API gateway, possibly the Understand integration service if it was separate, etc.) was built into a Docker image. This ensured that our application could run in a consistent environment across different machines (which is especially helpful for the teammates working on front-end to run the backend easily, and for deployment). I also wrote Dockerfiles and a docker-compose configuration so we could run the whole stack (including perhaps a database for tokens and the environment needed for the Python script) with a single command. While we did not fully deploy the system to a cloud platform during the course of the capstone, having it containerized means we are a step closer to that and can eventually deploy to a service like AWS or even package the extension with a local backend runtime.
- **Documentation & Reporting:** Throughout the project, I contributed extensively to our documentation – including the requirements, design diagrams, and weekly progress reports. As we iterated on the architecture (drawing C4 model diagrams for context and container layers) and updated our plans, I often took the lead in documenting those changes. This not only helped us stay aligned as a team but also served as a basis for our advisor updates and final report. Writing the documentation was a parallel responsibility that gave me a chance to reflect on our design decisions and ensure our approach was sound.
- **Testing (Sprint 3 Onward):** In the later stage of development, especially starting in Sprint 3, I spearheaded our testing efforts. This included writing unit tests for the backend logic – for example, testing that our summary generation service correctly calls the AI API and handles various responses, and that our refactoring detection pipeline correctly identifies known refactoring in sample commits. I also worked on integration testing: running the extension and backend together to simulate real-world use (for instance, authenticating with a dummy GitHub account, generating a commit, and verifying the extension displays the summary and metrics as expected). We also performed some security testing, such as ensuring API keys and tokens aren’t exposed and that our rate limiting (especially for external APIs like OpenAI) was handled gracefully. This testing phase was critical in catching last-minute bugs and ensuring the product was stable enough for a demo.

In summary, my role encompassed full-stack backend development – from initial architectural design and setting up the core services, through integrating complex external systems, to writing documentation and tests to polish the final product. It was a challenging role that required wearing multiple hats, but it was incredibly rewarding to see all the pieces come together.

## Backend Development Journey

Developing the backend for Commit Pro was an iterative journey that mirrored our project’s evolution through each sprint. Here, I’ll delve deeper into how the backend came to life and highlight some important milestones and technical decisions.

**Initial Setup and Architecture:** We began by establishing a solid foundation for the backend. Using Spring Boot, I bootstrapped a new project that would serve as our **API Gateway**. Early on, we defined the data models and API endpoints we anticipated needing. For instance, we knew we’d need endpoints like `/api/summary/{commitId}` to get an AI-generated summary for a commit, `/api/refactor/{commitId}` to get refactoring details, `/api/metrics/{repoId}` for code metrics, and authentication-related endpoints for connecting a GitHub account or providing an API key for the LLM. Designing these with a RESTful approach made the front-end integration straightforward down the line.

We also had to decide how to modularize the backend’s functionalities. One approach was a microservices style – e.g., separate services for summary generation, metrics, etc., all behind the API gateway. Given our project scope and time, we opted for a simpler approach: one primary service (the Spring Boot application) containing the necessary modules, and leveraging external tools via their APIs/CLI. We did, however, implement a separate **authentication/authorization module** within the backend to handle user tokens securely. This module managed GitHub OAuth token exchanges and stored credentials in a secure manner (for example, encrypted storage or in-memory for our development purposes).

**Integrating the AI for Commit Summaries:** The core feature of Commit Pro – generating commit message summaries – was one of the first things we tackled. I created a service in our backend that takes a commit’s data (diff, original commit message, author, timestamp, etc.) and constructs a prompt for the AI. Initially, our prompt simply concatenated the diff and asked the AI to summarize it. The results were mediocre: the summaries were sometimes too verbose or missed the point of the change. This led to a series of refinements:
- I added context to the prompt such as: *“The following code changes were made. Summarize the intent and impact in 1-2 sentences.”* We found that explicitly asking for intent and impact yielded more insightful summaries.
- We also tried specifying format, e.g., *“Start your summary with an action verb and mention the module or function names if relevant.”* This consistency was aimed at making summaries uniform and useful.
- As mentioned earlier, we experimented with the Spring AI integration which would have allowed using different LLM providers interchangeably. But due to time constraints and the overhead that library introduced, we decided to stick with calling OpenAI’s API directly via an HTTP client. This was a valuable lesson: sometimes a simpler integration is preferable during early development, and one can abstract/improve it later if needed.
- To ensure performance, I implemented asynchronous calls where possible. The commit summary generation call to OpenAI’s API could take a couple of seconds, so our backend would accept the request, immediately respond with a pending status (or perhaps kept the HTTP connection open using Spring’s async capabilities/WebFlux) and meanwhile fetch the result. We considered queuing or pre-fetching strategies if needed (e.g., generating summaries right after a commit is pushed, rather than waiting for the user to view it), but given the interactive nature of our extension, we stuck to on-demand generation with careful prompt optimizations.

**Handling Code Metrics via Understand:** One significant backend component was the generation of repository metrics for the dashboard. We integrated the Understand tool since it provides a wide range of static analysis metrics. The integration turned out trickier than expected. My approach was:
1. When the dashboard requests metrics (say, the user opens the dashboard for a repository), the backend will ensure we have an updated analysis of that repository. This might involve pulling the latest code (if not already stored) – for our prototype, we often worked with a sample repository or the user’s current repository.
2. I wrote a Python helper script to run Understand’s analysis. This script would:
   - Invoke Understand on the repository to update or create its internal database (.und file).
   - Run a series of CLI commands or API calls to extract metrics like total lines, complexity per function, class-level metrics (LCOM, etc.), and generate a report.
   - Print these metrics in a machine-readable format (JSON or CSV).
3. Back in the Java service, I call this Python script (using `ProcessBuilder` to execute the script with necessary arguments). I capture the output and then parse it into our own data model (a POJO containing all the metrics).
4. That data is then sent back to the dashboard request as JSON.

We had to iterate on this because at first, the metrics were coming back zero. I discovered (after some troubleshooting) that the educational license of Understand wouldn’t output metrics via automated means. The solution was to adjust how we call it (as described earlier, a hybrid approach of generating the .und file via CLI and reading from it). Eventually, we got it working: the dashboard could display real metrics for the code, which was a big win for the project’s proof of concept. This part of development taught me a lot about interoperability: gluing together a Java backend, a Python script, and a third-party analysis tool in a way that’s hidden from the end user but still reliable.

**Refactoring Insights Workflow:** For the refactoring detection, beyond the authentication hurdle I solved with shallow cloning, I made sure the performance was acceptable. Cloning a repository (even shallowly) on every request could be expensive. So, I implemented a basic caching mechanism: if the backend had recently analyzed a particular commit for refactoring, it would cache the results (in memory, or on disk) for a short duration. In practice, a user might trigger the analysis for a commit only once (when they view that commit), but if multiple team members or multiple views happened, caching prevented duplicate work. The output from RefactoringMiner included things like "Renamed Method X to Y in Class Z" or "Extracted Class A from Class B". Our backend formatted these into a human-readable summary that we included alongside the AI summary. For instance, if a commit just renames a function, the AI might not mention it explicitly, but our tool would add a note like "*Refactoring detected: Renamed function `oldName` to `newName`.*"

**Security and Reliability Considerations:** An important part of the backend journey was making sure our integrations were secure and robust:
- We ensured that any API keys or tokens (GitHub OAuth token, OpenAI API key provided by user) were not logged or exposed. I configured our logging to sanitize these and we stored them securely on the backend. For development we might keep them in memory, but a real deployment would use a secure vault or database with encryption.
- We implemented basic rate limiting on our API usage. For example, we knew the OpenAI API had a rate limit and cost, so we avoided calling it too frequently. If a user rapidly clicked a button to regenerate a summary, we might enforce a short cooldown. Similarly, we used GitHub’s conditional requests where possible to avoid hitting their rate limits (like using ETags or checking the last modified time of data).
- Error handling was another focus. I added graceful handling for cases such as: the OpenAI API is unreachable or returns an error (our backend would catch it and return a friendly error message or fallback), or Understand fails to run on a given repository (we would catch that and perhaps suggest the user to try again or check if the repository is too large).

**Iteration and Improvement:** Each sprint allowed us to improve the backend:
- By the end of **Sprint 1**, we had the basic skeleton: the extension could hit our API and get an AI summary for a commit, and we had preliminary support for GitHub auth and perhaps one metric or two.
- During **Sprint 2**, we added the heavy features: fully integrating the Understand metrics and RefactoringMiner, and fleshing out the dashboard API. We also faced and solved the integration issues of connecting the React dashboard within the extension environment (more on that in the next section about team coordination).
- In **Sprint 3**, our focus shifted to optimization and testing. I profiled parts of the backend to find any slow points (for example, the first call to the AI API was slower due to loading overhead, which we mitigated by a warm-up call). We also ensured that the backend could handle multiple requests in parallel without issues, as in a demo multiple commits might be analyzed quickly.

By the end of development, the backend of Commit Pro had evolved into a robust engine powering the entire application. It was extremely satisfying to see that a commit made by a developer would trigger a chain of events across various systems and within seconds yield a helpful summary and metrics – all of which our backend orchestrated. This journey taught me not just specific technologies, but also the art of integrating multiple moving parts into one cohesive system.

## Agile Process & Team Coordination

Our team followed an **Agile Scrum** approach throughout the project, which was crucial in managing a multidisciplinary project like Commit Pro. We organized our work into three sprints (roughly corresponding to three months of the semester), with each sprint focusing on a set of objectives and deliverables. 

We made heavy use of Jira for task management and sprint planning. At the start of the project, we brainstormed and wrote user stories covering all aspects of Commit Pro – from a developer’s perspective (“As a developer, I want an AI-generated summary of my commit so that I can save time writing commit messages”) to a project manager’s perspective (“...so I can get better project documentation”), etc. These user stories were broken down into tasks and bugs which we tracked on the Jira board. Each sprint, we picked a subset of these tasks as our sprint backlog, aiming to deliver incremental value.

![Jira Board Sprint 1](assets/screenshot-sprint1.png)  
*Snapshot of our Jira board at the end of Sprint 1, showing user stories and tasks. Each column represents a status (To Do, In Progress, Done) helping us visualize progress.*

In **Sprint 1**, our focus was on research and getting a working prototype for the core functionality. We divided tasks such that frontend development (browser extension basics) started in parallel with backend setup. I coordinated with the frontend developer (and our full-stack teammate) to define the API contract early – essentially agreeing on what endpoints the backend would provide and what JSON data structures we would use. We held short stand-up meetings (3 times a week) to sync up, discuss what we did, and any blockers. By the end of Sprint 1, we had a basic extension calling a basic backend – a big milestone that we demonstrated in our Sprint 1 review.

Sprint 1 taught us a lot about our velocity and teamwork. In the Sprint 1 review meeting with our advisor, we reflected on what went well (e.g., good division of labor, everyone had something tangible to show) and what needed improvement (e.g., our commit summary quality was still low, and we needed to gather more data on code metrics tools). We adjusted our plans for the next sprint accordingly.

**Sprint 2** was about expanding features and integration. We added more complex tasks to the board: implementing the dashboard UI, integrating the Understand and RefactoringMiner into the backend, improving the AI summaries, and dealing with any technical debt from Sprint 1. This sprint presented a challenge in team coordination, especially when the work of one sub-team (frontend or backend) depended on the other. For instance, as the frontend team built the React dashboard, they needed API endpoints ready and working. To facilitate this, I sometimes wrote stub responses in the backend (returning dummy data) so that the frontend could continue developing and not be blocked. We then iteratively replaced the stubs with real integration as the features became available. We also used a shared Git repository (on GitHub) and established a workflow (feature branches and pull requests). I often reviewed pull requests from others (especially those that touched the backend or documentation) and they did the same for my contributions. Code reviews and frequent merges helped prevent large merge conflicts and kept us all aware of changes across the codebase.

Midway through Sprint 2, we encountered integration issues when combining the Chrome extension with the new dashboard. Essentially, we had two separate React applications (one for the little popup/embedded extension UI, and one for the full dashboard) that needed to coexist. Our Jira board lit up with bug tickets like “Dashboard not loading inside extension” or “Styles clashing between extension and dashboard”. The team swarmed on these issues: our frontend lead fixed CSS and routing issues, and I assisted by tweaking how the extension serves the dashboard (e.g., updating the Chrome extension manifest to allow the dashboard’s files and using a HashRouter for the React app to work within a static context). We worked closely together, testing in real time and exchanging ideas for fixes. It was a hectic few days, but by the end we resolved the major integration blockers (as noted in our Sprint 2 review).

![Jira Board Sprint 2](assets/screenshot-sprint2.png)  
*Jira board during Sprint 2, with tasks related to integrating the dashboard, refining AI prompts, and adding code metrics. We continuously updated task status, and added new issues as we discovered bugs during development.*

Throughout these sprints, our adaptation of Agile was somewhat flexible due to academic timelines, but it served us well. We had **weekly meetings with our faculty advisor** where we treated them like Sprint reviews – demonstrating progress and getting feedback. We also did informal retrospectives to discuss what to improve in the next iteration. Communication was key: aside from Jira, our team stayed in touch via a group chat for quick questions and remote calls when needed (especially important when debugging together or planning out integration work).

By **Sprint 3**, our focus shifted to polishing and testing (as planned in our roadmap). We wrote more test cases, fixed bugs, and improved performance. The coordination here was about quality assurance – team members would pick up each other’s components to test. For example, the frontend folks tested some backend edge cases by trying unusual inputs, and I tested the extension’s usability by using it as a new user and seeing if the flows (like first-time authentication) were smooth. We logged any bugs on Jira and addressed them promptly since the end of the project was nearing.

In terms of team roles, we all had our specialties (frontend, backend, etc., as defined early on), but we maintained a very collaborative atmosphere. If the frontend needed a quick change on the backend, I would prioritize that, and likewise, if I needed some changes or additions on the frontend for better integration, the respective team members took it up. We avoided the “throw it over the wall” anti-pattern by constantly communicating. As a result, even though I was the backend developer, I had a decent understanding of the extension and dashboard code, and our frontend/fullstack members also understood the basics of our backend. This cross-functional awareness was fostered by our Agile process and made our final integration and testing much smoother.

Using agile methodologies and consistent team coordination, we managed to deliver a complex project on time. The Jira board screenshots above give a glimpse of how we tracked progress. We ended each sprint with most tasks moved to “Done,” and any spillover tasks were re-evaluated for necessity in the next sprint. This process kept us focused and accountable, and it was instrumental in the successful completion of Commit Pro.

## Challenges

No capstone project is without its challenges, and Commit Pro had its fair share. Over the course of development, we encountered technical hurdles and learning curves that we had to overcome as a team. Here are some of the major challenges we faced and how we addressed them:

- **Ensuring High-Quality AI Summaries:** *Challenge:* The first versions of AI-generated commit summaries were not as useful as we hoped. The model sometimes produced generic or incomplete descriptions, failing to clearly convey the intent of the code changes (especially if the commit was complex).  
  *Solution:* We tackled this by iterating on our prompt design and exploring advanced techniques. We experimented with **prompt engineering** – adding specific instructions to the prompt and even trying a method called Retrieval-Augmented Generation (RAG), where additional context (like relevant code or commit history) is provided to the model. These experiments, along with switching among AI models, gradually improved the summary quality. We found that a concise prompt asking for a one-sentence summary highlighting *“what and why”* of the change gave the best results. By the final iteration, the summaries were much more on-point, clearly describing the code modifications and their purpose.

- **Integration of Diverse Components:** *Challenge:* Commit Pro’s architecture involves a Chrome extension, a React web app, a Java backend, and external tools (some running via Python). Integrating all these moving parts was complex. In particular, getting the **React dashboard to work within the Chrome extension** environment was an unexpected headache. We encountered problems with routing (the dashboard links not loading properly inside the extension), styling conflicts between the extension popup and the dashboard, and even Chrome-specific security restrictions that blocked certain files from loading.  
  *Solution:* Overcoming this required careful debugging and some architecture adjustments. We decided to use a **HashRouter** in the React app, which is more compatible with static file serving (like in an extension) compared to HTML5 routing. We restructured the extension to serve the dashboard’s build files from an embedded `dashboard` folder, ensuring all assets were included and permitted in the Chrome manifest. To fix styling issues, the team modularized CSS and made sure the dashboard’s styles were properly isolated. We also added a build step to our pipeline that would automatically compile and copy the latest dashboard code into the extension directory before packaging. This tight integration was tricky, but once solved, it meant we had a single extension that contained both the lightweight popup UI and the richer dashboard, working seamlessly together.

- **External Tool Limitations (Understand License):** *Challenge:* Using the Understand tool for metrics introduced a non-coding challenge: license restrictions. The educational license available to us did not support programmatic export of certain metrics, which we only discovered after implementation. This meant our initial approach to get metrics via the official API failed silently, leaving us perplexed as to why we were getting zeros for everything.  
  *Solution:* We approached this systematically. We consulted forums and documentation which hinted at the limitations of the license. Instead of giving up on the metrics feature, we improvised a workaround. I wrote a script to **capture CLI output** of the metrics generation, essentially treating the tool like a black box and scraping its textual output. This was not the most elegant solution, but it worked under our license constraints. We also made note that in a production scenario, obtaining a full license or using an alternative open-source metrics library would be the way to go. The key lesson was adaptability – when faced with a limitation outside our control, we found a creative solution to still achieve the end result.

- **Authentication and Security Challenges:** *Challenge:* Our system needed to interact with user accounts (GitHub, and use an API key for the LLM service). Handling authentication flows in a seamless yet secure manner was challenging. For instance, when a user uses Commit Pro for the first time, they should be prompted to connect their GitHub account. Doing this through a Chrome extension (which has a limited UI) and linking it to our backend’s OAuth logic was complex. Additionally, securely storing the GitHub token once acquired (so the user doesn’t have to log in every time) and similarly storing the user’s OpenAI API key required careful thought to avoid exposing sensitive data.  
  *Solution:* We implemented the GitHub OAuth by launching a separate web page for login (GitHub’s OAuth page) and then redirecting to our backend’s callback URL. The backend would receive a code, exchange it for a token, and then send a message back to the extension indicating success. This dance between extension and backend was hard to get right, but we eventually did through trial and error and by using Chrome extension messaging APIs. For storage, during development, we simply kept the token in memory or extension local storage (for demo purposes). In a real setting, the backend would store the token in a database, encrypted. We ensured that all API communications were over HTTPS (when deployed) to protect the tokens in transit. We also educated ourselves on Chrome extension content security policies to ensure we weren’t unintentionally creating vulnerabilities (like we had to be mindful of not injecting any dangerous scripts in the extension, etc.). By the end, the authentication flow was smooth: a user clicks “Connect GitHub” in the extension, goes through GitHub login, and then Commit Pro is ready to use.

- **Time Constraints and Scope Management:** *Challenge:* As with any capstone, we had a fixed timeline and a lot we wanted to deliver. Around mid-project, we realized some features might be too ambitious (for example, supporting multiple AI models or advanced analytics beyond what Understand provides). It was a challenge to prioritize and sometimes cut features to ensure we delivered a working product by the deadline.  
  *Solution:* We leaned on our Agile process to manage this. By maintaining a clear product backlog in Jira and marking which items were “Must-have” vs “Nice-to-have,” we kept our focus on core functionality first. Features like multi-model support for the AI were deprioritized in favor of making the single-model solution robust. We also allocated the final sprint mostly to testing and polish rather than new features, which was a tough call but absolutely the right one. This ensured that what we did deliver, we delivered well. In retrospect, this challenge taught us about realistic planning and the importance of a minimal viable product (MVP) mindset in project management.

Each of these challenges pushed us to learn and adapt. Overcoming them not only made Commit Pro a better project but also gave us invaluable experience in problem-solving, teamwork, and perseverance under pressure.

## Lessons Learned

Working on Commit Pro was a tremendous learning experience. Beyond the technical knowledge gained in building an end-to-end software solution, I walked away with several key lessons:

- **The Power of Iterative Development:** We truly saw the benefits of an agile, iterative approach. By building the project in increments (and having something demo-able early on), we could gather feedback and uncover issues sooner rather than later. For example, having a basic commit summary working in Sprint 1 allowed us to realize its shortcomings and spend the next sprints improving it. This reinforced for me the importance of getting a prototype out quickly and then refining it, rather than trying to build the “perfect” solution in one go.

- **Importance of Communication and Teamwork:** Our project’s success was largely due to effective team coordination. I learned that frequent communication – be it stand-ups, update messages, or code review discussions – prevents a lot of problems. When we hit the tricky integration bug with the dashboard and extension, it was the collective brainstorming and willingness to help across roles that solved it. This experience taught me how crucial it is to be both a good listener and a proactive communicator on a software team. No matter how skilled each individual is, if we don’t communicate well, the project can suffer.

- **Adaptability in the Face of Technical Roadblocks:** Throughout Commit Pro, we encountered unexpected roadblocks (license issues, library limitations, etc.). A big lesson learned was to stay adaptable and resourceful. When one approach failed, we tried another; when a tool didn’t work as expected, we looked for alternatives or workarounds. I personally learned to not get discouraged by initial failures. Debugging the Understand tool integration, for example, took a lot of head-scratching, but it taught me to systematically analyze a problem, consult documentation/community, and think outside the box for solutions. This adaptability is a skill I’ll carry forward to any project.

- **Balancing Ambition with Practicality:** As students and developers, we were excited to pack as many features as possible into Commit Pro. But we learned to balance that enthusiasm with pragmatism. There were moments we had to decide to reduce scope or delay a feature for a future version in order to solidify what was already in progress. I learned the value of an MVP – identifying the core value of the project and making sure that is delivered reliably, rather than an over-ambitious project that might collapse under its own complexity. This lesson in scope management and prioritization is something that will be very useful in my future projects and career.

- **Testing Early and Often:** In hindsight, we could have started our testing (especially automated testing) earlier. When we did begin writing comprehensive tests in Sprint 3, we immediately caught bugs and regressions that might have been lurking unnoticed. This drove home the lesson that testing isn’t something to slap on at the end, but rather an integral part of development. Even though we were students with limited time, investing in test cases saved us time debugging later. I will remember to advocate for building a test suite alongside the code in future projects, even if under time pressure.

- **Value of Documentation:** At times, writing documentation felt like a chore when there was exciting coding to do. However, keeping our design document, user guides, and Jira tickets up to date paid dividends. It helped onboard a team member who got stuck on someone else’s part, it helped in discussions with our advisor, and now it’s helping me write this very blog post with clarity about what we did! This reinforced how proper documentation is part of delivering a professional product and aids knowledge transfer and maintenance.

In summary, the capstone project was as much about personal and professional growth as it was about delivering a software tool. I learned how to think critically and holistically as an engineer – considering not just writing code, but designing for users, working in a team, managing time, and ensuring quality. These lessons have prepared me for the next challenges in my career.

## Conclusion & Next Steps

Working on Commit Pro has been a rewarding journey. In the span of a few months, our team took an idea – using AI to enhance commit documentation – and turned it into a working product with real impact. The final result is a Chrome extension that seamlessly integrates with GitHub to provide developers with AI-generated commit summaries and refactoring insights, complemented by a rich dashboard of code metrics. We were able to demonstrate how Commit Pro can save developer time, improve the clarity of project history, and offer additional analytics that can be valuable for project management and QA. It was incredibly fulfilling to see our demo come together: making a commit on a test repository and watching Commit Pro instantly display a concise summary of the changes and even point out a refactoring we did.

That said, software projects are never truly “finished” – there are always improvements and next steps on the horizon. For Commit Pro, a few promising next steps and future enhancements include:

- **Public Release:** We plan to refine the project further and release Commit Pro to a broader audience. This involves polishing the user experience and then publishing the extension on the Chrome Web Store so others can easily install it. Alongside, we would host our backend (perhaps on a cloud service) or provide instructions for users to deploy it themselves. An open-source release on GitHub is also on the table, inviting other developers to try the tool and even contribute to its development.

- **Enhanced AI Models:** While our current implementation works with a general-purpose AI model, we see potential in training or fine-tuning an AI specifically for commit messages. In the future, we could incorporate a model that has learned from thousands of high-quality commit messages, which might produce even better summaries. Also, adding support for multiple models (and model selection) could let users choose a faster model vs. a more accurate one based on their needs.

- **Deeper Analytics:** We scratched the surface with repository metrics. A next step is to provide more insights on the dashboard – for example, trend lines of code growth, “hotspots” in the code (files that change often or have high complexity), or even predictive insights like “this module has a high bug rate historically.” These would further enhance the value of Commit Pro for managers and team leads. We also considered integrating other tools or databases; for instance, linking commit data with issue trackers (like Jira) to see if a commit is related to a particular bug or story.

- **User Feedback Integration:** Incorporating a feedback loop could make the AI summaries more accurate over time. For example, allowing developers to rate or edit the AI-generated commit message, and feeding that data back to improve the system. This could be done via the extension UI (a simple thumbs-up/down on the summary). Over time, this could help identify common shortcomings of the AI and address them, either through prompt adjustments or model improvements.

- **Cross-Platform Support:** Currently, Commit Pro is a Chrome extension for GitHub. A natural next step is to consider support for other browsers (Firefox, etc.) and possibly other version control platforms like GitLab or Bitbucket. The core backend logic would remain largely the same, but we’d adapt the extension portion to different environments. This would expand the reach of our tool to more developers.

As we wrap up this capstone project, I feel proud of what our team accomplished and excited about where Commit Pro could go next. Personally, I’m coming out of this project with new skills and experiences that I will carry into my professional career. From backend development and systems integration to agile teamwork and problem-solving, the journey was filled with growth opportunities. 

Commit Pro started as an academic project, but we envision it evolving into a practical tool that developers can rely on in real-world projects. The coming months may see us turning this prototype into a polished product. No matter what, the story of Commit Pro – and my journey in building it – will remain a highlight of my final year in university. 

**Thank you for reading!** If you’re interested in Commit Pro or have suggestions, feel free to reach out or check out the project’s repository. This project has shown me the impact that a small, focused team can make, and I’m excited to apply these learnings to future endeavors.
